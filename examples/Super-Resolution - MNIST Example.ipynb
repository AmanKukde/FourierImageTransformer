{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/envs/FIT_ls/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/localscratch/envs/FIT_ls/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail23torchInternalAssertFailEPKcS2_jS2_RKSs'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/localscratch/envs/FIT_ls/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from fit.datamodules.super_res import MNIST_SResFITDM\n",
    "from fit.utils.tomo_utils import get_polar_rfft_coords_2D\n",
    "import datetime\n",
    "from fit.modules.SResTransformerModule import SResTransformerModule\n",
    "import wandb \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from fit.transformers.PSNR import RangeInvariantPsnr as PSNR\n",
    "from os.path import exists\n",
    "import wget\n",
    "import ssl\n",
    "import torchvision\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 22122020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22122020"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(22122020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNIST_SResFITDM(root_dir='./datamodules/data/', batch_size=8)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, phi, flatten_order, order = get_polar_rfft_coords_2D(img_shape=dm.gt_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_heads = 8\n",
    "d_query = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SResTransformerModule(d_model=n_heads*d_query, \n",
    "                              img_shape=dm.gt_shape,\n",
    "                              coords=(r, phi),\n",
    "                              dst_flatten_order=flatten_order,\n",
    "                              dst_order=order,\n",
    "                              loss='prod',\n",
    "                              lr=0.0001, weight_decay=0.01, n_layers=8,\n",
    "                              n_heads=n_heads, d_query=d_query, dropout=0.1, attention_dropout=0.1,num_shells = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=100, \n",
    "                  #gpus=1, # set to 0 if you want to run on CPU\n",
    "                  callbacks=ModelCheckpoint(\n",
    "                                            dirpath=None,\n",
    "                                            save_top_k=1,\n",
    "                                            verbose=False,\n",
    "                                            save_last=True,\n",
    "                                            monitor='Validation/avg_val_loss',\n",
    "                                            mode='min'\n",
    "                                        ), \n",
    "                  deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_test_model('/home/aman.kukde/Projects/FourierImageTransformer/models_saved/04-03_23-23-38_sum_+/epoch=168-step=290511.ckpt')\n",
    "# model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc, (mag_min, mag_max) in dm.test_dataloader():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = fc.to('cuda')\n",
    "mag_min = mag_min.to('cuda')\n",
    "mag_max = mag_max.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 7.82 GiB of which 60.75 MiB is free. Process 5955 has 442.16 MiB memory in use. Process 2772434944 has 0 bytes memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 527.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x_fc \u001b[38;5;241m=\u001b[39m fc[:, flatten_order][:, :\u001b[38;5;241m96\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_i\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fc\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m pred_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert2img(fc\u001b[38;5;241m=\u001b[39mpred, mag_min\u001b[38;5;241m=\u001b[39mmag_min, mag_max\u001b[38;5;241m=\u001b[39mmag_max)\n",
      "File \u001b[0;32m~/Projects/FourierImageTransformer/examples/../fit/transformers/SResTransformer.py:76\u001b[0m, in \u001b[0;36mSResTransformerTrain.forward_i\u001b[0;34m(self, x, input_seq_length)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     75\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfourier_coefficient_embedding(x)\n\u001b[0;32m---> 76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(input_seq_length,\u001b[38;5;241m377\u001b[39m):\n\u001b[1;32m     78\u001b[0m         y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x[:,:i,:])\n",
      "File \u001b[0;32m/localscratch/envs/FIT_ls/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/localscratch/envs/FIT_ls/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/FourierImageTransformer/examples/../fit/transformers/PositionalEncoding2D.py:61\u001b[0m, in \u001b[0;36mPositionalEncoding2D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m pos_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpe[:, :x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m), :]\n\u001b[1;32m     60\u001b[0m pos_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrepeat_interleave(pos_embedding, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_embedding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacty of 7.82 GiB of which 60.75 MiB is free. Process 5955 has 442.16 MiB memory in use. Process 2772434944 has 0 bytes memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 527.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "x_fc = fc[:, flatten_order][:, :96]\n",
    "pred = model.sres.forward_i(x_fc,96)\n",
    "\n",
    "pred_img = self.convert2img(fc=pred, mag_min=mag_min, mag_max=mag_max)\n",
    "\n",
    "plt.imshow(pred_img[0].cpu().detach().numpy(), cmap='gray')\n",
    "plt.savefig('pred_img_aman.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PSNR(gt,lowres) - PSNR(gt,pred_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in range(10):\n",
    "    fig = plt.figure(figsize=(31/2., 10/2.)) \n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[10,0.5, 10, 0.5, 10]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[2])\n",
    "    ax2 = plt.subplot(gs[4])\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                        hspace = 0, wspace = 0)\n",
    "\n",
    "    ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.imshow(lowres[sample], cmap='gray', vmin=gt[sample].min(), vmax=gt[sample].max())\n",
    "    ax0.set_title('Low-Resolution Input');\n",
    "    ax0.axis('equal');\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.imshow(pred_img[sample], cmap='gray', vmin=gt[sample].min(), vmax=gt[sample].max())\n",
    "    ax1.set_title('Prediction');\n",
    "    ax1.axis('equal');\n",
    "\n",
    "\n",
    "    ax2.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.imshow(gt[sample], cmap='gray')\n",
    "    ax2.set_title('Ground Truth');\n",
    "    ax2.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = []\n",
    "lowres_scaled = torch.zeros_like(lowres)\n",
    "pred_img_scaled = torch.zeros_like(pred_img)\n",
    "for i in range(4):\n",
    "    sample = i\n",
    "    # pred_img_scaled[i] = (pred_img[i] - gt[i].min())/(gt[i].max() - gt[i].min())\n",
    "    # lowres_scaled[i] = (lowres[i] - gt[i].min())/(gt[i].max() - gt[i].min())\n",
    "    fig = plt.figure(figsize=(31/2., 10/2.))\n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[10,0.5, 10, 0.5, 10]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[2])\n",
    "    ax2 = plt.subplot(gs[4])\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                        hspace = 0, wspace = 0)\n",
    "    ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.yaxis.set_major_locator(plt.NullLocator())\n",
    "    # lowres_scaled_vs_gt_psnr = PSNR(gt[sample][2:, 2:], lowres_scaled[sample][2:, 2:], drange=torch.tensor(255., dtype=torch.float32))\n",
    "    ax0.imshow(np.roll(abs(torch.fft.rfftn(lowres[sample],dim = [0,1])),13,0))\n",
    "    ax0.set_title('Low-Resolution Input');\n",
    "    # ax0.set_xlabel(f'PSNR: {lowres_scaled_vs_gt_psnr:.2f} dB\\nMax: {lowres_scaled[sample].max():.2f} Min: {lowres_scaled[sample].min():.2f}');\n",
    "    ax0.axis('equal');\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.yaxis.set_major_locator(plt.NullLocator())\n",
    "    \n",
    "    ax1.imshow(np.roll(abs(torch.fft.rfftn(pred_img[sample],dim = [0,1])),13,0))\n",
    "    # pred_vs_gt_psnr = PSNR(gt[sample][2:, 2:], pred_img_scaled[sample][2:, 2:], drange=torch.tensor(255., dtype=torch.float32))\n",
    "    ax1.set_title('Prediction');\n",
    "    # ax1.set_xlabel(f'PSNR: {pred_vs_gt_psnr:.2f} dB\\nMax: {pred_img_scaled[sample].max():.2f} Min: {pred_img_scaled[sample].min():.2f}');\n",
    "    ax1.axis('equal');\n",
    "\n",
    "    ax2.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.imshow(np.roll(abs(torch.fft.rfftn(gt[sample],dim = [0,1])),13,0))\n",
    "    ax2.set_title('Ground Truth');\n",
    "    ax2.set_xlabel(f'Max: {gt[sample].max():.2f} Min: {gt[sample].min():.2f}');\n",
    "    ax2.axis('equal');\n",
    "    # diff.append(lowres_vs_gt_psnr - pred_vs_gt_psnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.zeros(3,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3,2,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy[:,:2,:] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
