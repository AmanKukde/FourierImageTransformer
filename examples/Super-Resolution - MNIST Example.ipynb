{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising model and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 22122020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SResTransformerModule(\n",
       "  (sres): SResTransformer(\n",
       "    (fourier_coefficient_embedding): Linear(in_features=2, out_features=128, bias=True)\n",
       "    (pos_embedding): PositionalEncoding2D(\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): MambaModel(\n",
       "      (embeddings): Embedding(50280, 256)\n",
       "      (layers): ModuleList(\n",
       "        (0-7): 8 x MambaBlock(\n",
       "          (norm): MambaRMSNorm()\n",
       "          (mixer): MambaMixer(\n",
       "            (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "            (act): SiLU()\n",
       "            (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "            (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "            (dt_proj): Linear(in_features=48, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_f): MambaRMSNorm()\n",
       "    )\n",
       "    (predictor_amp): Linear(in_features=256, out_features=1, bias=True)\n",
       "    (predictor_phase): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from fit.datamodules.super_res import MNIST_SResFITDM, CelebA_SResFITDM,MNIST_SResFITDM_Large,Omniglot\n",
    "from fit.utils.tomo_utils import get_polar_rfft_coords_2D\n",
    "\n",
    "\n",
    "from fit.modules.SResTransformerModule import SResTransformerModule\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from fit.utils.PSNR import RangeInvariantPsnr as PSNR\n",
    "# from fit.utils.utils import PSNR\n",
    "import torch\n",
    "import numpy as np\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from fit.utils.utils import denormalize\n",
    "\n",
    "trained_model_path = '/home/aman.kukde/Projects/FourierImageTransformer/models/CelebA/mamba/sum/Mamba_CelebA_wp_10000_sum_L_8_H_8_s_10_subset_False_Contd._26-04_13-14-02_29-04_17-56-08/epoch=1799-step=3892500.ckpt'\n",
    "# trained_model_path = '/home/aman.kukde/Projects/FourierImageTransformer/models/CelebA/fast/sum/Fast_CelebA_sum__wp_1000_L_8_H_8_s_5_subset_False_08-04_18-18-18/epoch=221-step=555000.ckpt'\n",
    "\n",
    "dataset = trained_model_path.split('/')[-5]\n",
    "model_type = trained_model_path.split('/')[-4]\n",
    "loss = trained_model_path.split('/')[-3]\n",
    "\n",
    "seed_everything(22122020)\n",
    "# dataset = \"omniglot\"\n",
    "if dataset == \"MNIST\":\n",
    "    dm = MNIST_SResFITDM(root_dir=\"./datamodules/data/\",\n",
    "                            batch_size=32, subset_flag=False)\n",
    "if dataset == \"MNIST_large\":\n",
    "    dm = MNIST_SResFITDM_Large(root_dir=\"./datamodules/data/\",\n",
    "                            batch_size=2, subset_flag=False)\n",
    "if dataset == \"CelebA\":\n",
    "    dm = CelebA_SResFITDM(root_dir=\"./datamodules/data/\",\n",
    "                            batch_size=4, subset_flag=False)\n",
    "if dataset == \"omniglot\":\n",
    "    dm = Omniglot(root_dir=\"./datamodules/data/\",batch_size=8, subset_flag=False)\n",
    "\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "r, phi, flatten_order, order = get_polar_rfft_coords_2D(img_shape=dm.gt_shape)\n",
    "n_heads = 8\n",
    "d_query = 32\n",
    "model = SResTransformerModule(img_shape=dm.gt_shape,\n",
    "                              coords=(r, phi),\n",
    "                              dst_flatten_order=flatten_order,\n",
    "                              dst_order=order,\n",
    "                              loss=loss,\n",
    "                              lr=0.0001, weight_decay=0.01, n_layers=8,\n",
    "                              n_heads=n_heads, d_query=d_query,num_shells = 4,\n",
    "                              model_type = model_type)\n",
    "\n",
    "\n",
    "weights = torch.load(trained_model_path)['state_dict']\n",
    "model.load_state_dict(weights, strict=True);print('Model Loaded')\n",
    "model.cuda()\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/1.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/2.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/3.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/4.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/5.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/6.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/7.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/8.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/9.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/10.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/11.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/12.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/13.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/14.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/15.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/16.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/17.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/18.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/19.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/20.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/21.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/22.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/23.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/24.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/25.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/26.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/27.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/28.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/29.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/30.png',\n",
       " '/home/aman.kukde/Projects/FourierImageTransformer/aman/img_1/31.png']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "natsorted(imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sequence Length: 1\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 13\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 39\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 75\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 121\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 185\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 257\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 347\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 447\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 561\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 677\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 821\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 967\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 1135\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 1309\n",
      "torch.Size([2016, 2])\n",
      "Input Sequence Length: 1497\n",
      "torch.Size([2016, 2])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_one_test_batch(num_images = 3,num_shells = model.shells):\n",
    "\n",
    "    x, y = np.meshgrid(\n",
    "                range(model.dft_shape[1]),\n",
    "                range(-model.dft_shape[0] // 2, model.dft_shape[0] // 2 + 1))\n",
    "    radii = np.roll(np.sqrt(x**2 + y**2, dtype=np.float32),model.dft_shape[0] // 2 + 1, 0)\n",
    "\n",
    "    input_seq_len = np.sum(np.round(radii) < num_shells)\n",
    "    \n",
    "    print(f\"Input Sequence Length: {input_seq_len}\")\n",
    "    \n",
    "    for fc, (mag_min, mag_max) in dm.test_dataloader():\n",
    "        break\n",
    "    n_imgs  = min(num_images,fc.shape[0])\n",
    "    fc = fc.to('cuda')\n",
    "    mag_min = mag_min.to('cuda')\n",
    "    mag_min = mag_min[:n_imgs].to('cuda')\n",
    "    mag_max = mag_max.to('cuda')\n",
    "    mag_max = mag_max[:n_imgs].to('cuda')\n",
    "    x_input = fc[:n_imgs,model.dst_flatten_order][:, :input_seq_len]\n",
    "\n",
    "    gt = fc[:n_imgs, model.dst_flatten_order]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model.sres.forward_inference(x_input,max_seq_length=gt.shape[1])\n",
    "    # pred[:,:,1] = gt[:,:,1]\n",
    "    # pred = torch.zeros_like(gt)\n",
    "   \n",
    "    lowres = torch.zeros_like(pred)\n",
    "    lowres[...,0] += x_input[...,0].min()\n",
    "    lowres[:, :input_seq_len] = x_input\n",
    "    pred_img = model.convert2img(fc=pred, mag_min=mag_min, mag_max=mag_max)\n",
    "    lowres_img = model.convert2img(fc=lowres, mag_min=mag_min, mag_max=mag_max)\n",
    "    gt_img = model.convert2img(fc=gt, mag_min=mag_min, mag_max=mag_max)\n",
    "\n",
    "    lowres_img = denormalize(lowres_img,dm.mean,dm.std)\n",
    "    pred_img = denormalize(pred_img,dm.mean,dm.std)\n",
    "    gt_img = denormalize(gt_img,dm.mean,dm.std)\n",
    "\n",
    "\n",
    "\n",
    "    lowres_psnr = PSNR(gt_img,lowres_img)\n",
    "    pred_psnr = PSNR(gt_img,pred_img)\n",
    "    gt_psnr = PSNR(gt_img,gt_img)\n",
    "    return pred_img, gt_img, lowres_img, lowres_psnr, pred_psnr, gt_psnr, num_images, input_seq_len\n",
    "\n",
    "def plot_images(pred_img, gt_img, lowres_img, lowres_psnr, pred_psnr, gt_psnr,no_of_images = 5,input_seq_len=5,shells = model.shells):\n",
    "\n",
    "    \n",
    "\n",
    "    for i in [pred_img, lowres_img, gt_img]:\n",
    "        i.cpu().detach().numpy()\n",
    "    # for sample in range(no_of_images):\n",
    "    sample = 1\n",
    "    for s in range(1):\n",
    "        fig2 = plt.figure(figsize=(62.5/3., 10/3.))\n",
    "    \n",
    "        gs = gridspec.GridSpec(1,5, width_ratios=[10,0.5, 10, 0.5, 10])\n",
    "        ax0 = plt.subplot(gs[0])\n",
    "        # ax1 = plt.subplot(gs[2])\n",
    "        ax2 = plt.subplot(gs[2])\n",
    "        # ax3 = plt.subplot(gs[6])\n",
    "        ax4 = plt.subplot(gs[4])\n",
    "        # gs = gridspec.GridSpec(1,9, width_ratios=[10,0.5, 10, 0.5, 10, 0.5, 10, 0.5, 10])\n",
    "        # ax0 = plt.subplot(gs[0])\n",
    "        # ax1 = plt.subplot(gs[2])\n",
    "        # ax2 = plt.subplot(gs[4])\n",
    "        # ax3 = plt.subplot(gs[6])\n",
    "        # ax4 = plt.subplot(gs[8])\n",
    "\n",
    "        plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0,\n",
    "                            hspace = 0, wspace = 0)\n",
    "\n",
    "        ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax0.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax0.imshow(lowres_img[sample].cpu(), cmap='gray')\n",
    "        ax0.set_title('Low-Resolution Input');\n",
    "        ax0.axis('equal');\n",
    "\n",
    "        ax2.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax2.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax2.imshow(pred_img[sample].cpu(), cmap='gray')\n",
    "        ax2.set_title('Prediction');\n",
    "        ax2.set_xlabel(f'Shells : {shells}, Input Sequence Length: {input_seq_len}')\n",
    "        ax2.axis('equal');\n",
    "\n",
    "\n",
    "        ax4.xaxis.set_major_locator(plt.NullLocator())\n",
    "        ax4.yaxis.set_major_locator(plt.NullLocator())\n",
    "        ax4.imshow(gt_img[sample].cpu(), cmap='gray')\n",
    "        ax4.set_title('Ground Truth');\n",
    "        ax4.axis('equal');\n",
    "\n",
    "        # ax1.xaxis.set_major_locator(plt.NullLocator())\n",
    "        # ax1.yaxis.set_major_locator(plt.NullLocator())\n",
    "        # lowres_psnr = PSNR(gt_img, lowres_img)\n",
    "        # ax1.imshow(np.roll(abs(torch.fft.rfftn(lowres_img[sample].cpu(),dim = [0,1])),model.dft_shape[0] // 2 ,0))\n",
    "        # ax1.set_title('Low-Resolution Input');\n",
    "        # ax1.set_xlabel(f'PSNR: {lowres_psnr[sample].cpu().detach()} dB')\n",
    "        # ax1.axis('equal');\n",
    "\n",
    "        # ax3.xaxis.set_major_locator(plt.NullLocator())\n",
    "        # ax3.yaxis.set_major_locator(plt.NullLocator())\n",
    "        # ax3.imshow(np.roll(abs(torch.fft.rfftn(pred_img[sample].cpu(),dim = [0,1])),model.dft_shape[0] // 2,0))\n",
    "        # pred_psnr = PSNR(gt_img, pred_img)\n",
    "        # ax3.set_title('Prediction');\n",
    "        # ax3.set_xlabel(f'PSNR: {pred_psnr[sample].cpu().detach()} dB')\n",
    "        # ax3.axis('equal');\n",
    "        plt.savefig(f'/home/aman.kukde/Projects/FourierImageTransformer/aman/img_2/{shells}.jpg')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n",
    "b = np.arange(1,32,2)\n",
    "for i in b:\n",
    "    pred_img, gt_img, lowres_img, lowres_psnr, pred_psnr, gt_psnr, no_of_images,input_seq_len = predict_one_test_batch(num_images = 6,num_shells=i)\n",
    "    plot_images(pred_img, gt_img, lowres_img, lowres_psnr, pred_psnr, gt_psnr, no_of_images,input_seq_len, shells = i)\n",
    "\n",
    "from PIL import Image\n",
    "from natsort import natsorted\n",
    "import glob\n",
    "\n",
    "# Create the frames\n",
    "frames = []\n",
    "imgs = glob.glob(\"/home/aman.kukde/Projects/FourierImageTransformer/aman/img_2/*.jpg\")\n",
    "for i in natsorted(imgs):\n",
    "    new_frame = Image.open(i)\n",
    "    frames.append(new_frame)\n",
    "\n",
    "# Save into a GIF file that loops forever\n",
    "frames[0].save('png_to_gif_2.gif', format='GIF',\n",
    "               append_images=frames[1:],\n",
    "               save_all=True,\n",
    "               duration=300, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
