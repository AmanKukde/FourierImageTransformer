{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising model and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from fit.datamodules.super_res import MNIST_SResFITDM\n",
    "from fit.utils.tomo_utils import get_polar_rfft_coords_2D\n",
    "from fit.utils.utils import denormalize, denormalize_amp, denormalize_phi\n",
    "\n",
    "from fit.modules.SResTransformerModule import SResTransformerModule\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from utils.PSNR import RangeInvariantPsnr as PSNR\n",
    "# from fit.utils.utils import PSNR\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from os.path import exists\n",
    "import wget\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "import scipy\n",
    "seed_everything(22122020)\n",
    "dm = MNIST_SResFITDM(root_dir='./datamodules/data/', batch_size=32,subset_flag = False)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "r, phi, flatten_order, order = get_polar_rfft_coords_2D(img_shape=dm.gt_shape)\n",
    "n_heads = 8\n",
    "d_query = 32\n",
    "model = SResTransformerModule(img_shape=dm.gt_shape,\n",
    "                              coords=(r, phi),\n",
    "                              dst_flatten_order=flatten_order,\n",
    "                              dst_order=order,\n",
    "                              loss='prod',\n",
    "                              lr=0.0001, weight_decay=0.01, n_layers=8,\n",
    "                              n_heads=n_heads, d_query=d_query,num_shells =5,\n",
    "                              model_type = 'fast')\n",
    "\n",
    "fig = plt.figure()\n",
    "psnr_dict = {}\n",
    "trained_model_path = '/home/aman.kukde/Projects/FourierImageTransformer/models/Fast_prod__L_8_H_8_s_5_subset_False_27-03_16-58-36/epoch=222-step=383337.ckpt'\n",
    "# trained_model_path = '/home/aman.kukde/Projects/FourierImageTransformer/models/Fast_prod__L_8_H_8_s_5_subset_False_26-03_20-10-16/epoch=183-step=316296.ckpt'\n",
    "# trained_model_path = '/home/aman.kukde/Projects/FourierImageTransformer/models/Fast_sum__L_8_H_8_s_5_subset_False_27-03_19-06-54/epoch=261-step=450378.ckpt'\n",
    "weights = torch.load(trained_model_path)['state_dict']\n",
    "# model.load_state_dict(weights)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "trainer = Trainer(max_epochs=100, \n",
    "                #gpus=1, # set to 0 if you want to run on CPU\n",
    "                callbacks=ModelCheckpoint(\n",
    "                                            dirpath=None,\n",
    "                                            save_top_k=1,\n",
    "                                            verbose=False,\n",
    "                                            save_last=True,\n",
    "                                            monitor='Validation/avg_val_loss',\n",
    "                                            mode='min'\n",
    "                                        ), \n",
    "                deterministic=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading DataModule and predicting using inference and forward function both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/localscratch/miniforge3/envs/SSM_Home/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "input_ids = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "out = model.generate(input_ids, max_new_tokens=10)\n",
    "print(tokenizer.batch_decode(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc, (mag_min, mag_max) in dm.test_dataloader():\n",
    "    break\n",
    "\n",
    "fc = fc.to('cuda')\n",
    "mag_min = mag_min.to('cuda')\n",
    "mag_max = mag_max.to('cuda')\n",
    "low_res = fc[:, flatten_order][:, :model.input_seq_length]\n",
    "x_fc = fc[:, flatten_order][:, :-1]\n",
    "y_fc = fc[:, flatten_order][:, 1:]\n",
    "gt = fc[:, flatten_order]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"conv_kernel\": 4,\n",
       "  \"d_model\": 256,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"expand\": 1,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 512,\n",
       "  \"initializer_range\": 0.1,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"mamba\",\n",
       "  \"num_hidden_layers\": 2,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rescale_prenorm_residual\": false,\n",
       "  \"residual_in_fp32\": true,\n",
       "  \"state_size\": 16,\n",
       "  \"time_step_floor\": 0.0001,\n",
       "  \"time_step_init_scheme\": \"random\",\n",
       "  \"time_step_max\": 0.1,\n",
       "  \"time_step_min\": 0.001,\n",
       "  \"time_step_rank\": 48,\n",
       "  \"time_step_scale\": 1.0,\n",
       "  \"transformers_version\": \"4.40.0.dev0\",\n",
       "  \"use_bias\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_conv_bias\": true,\n",
       "  \"vocab_size\": 2\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 3.77 M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NewMamba(\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x MambaBlock(\n",
       "      (norm): MambaRMSNorm()\n",
       "      (mixer): MambaMixer(\n",
       "        (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "        (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "        (dt_proj): Linear(in_features=48, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers.models.mamba.modeling_mamba import MambaBlock\n",
    "from transformers import MambaConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "class NewMamba(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conf = MambaConfig()\n",
    "        conf.num_hidden_layers = 8\n",
    "        conf.expand = 4\n",
    "        conf.hidden_size = 256\n",
    "        # conf.state_size = 32\n",
    "        conf.intermediate_size = 512\n",
    "        self.layers = torch.nn.ModuleList([MambaBlock(conf, layer_idx=x) for x in range(8)])\n",
    "    def forward(self, x):\n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "model = NewMamba()\n",
    "out = model.forward(torch.randn(2, 377, 256))\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Number of parameters: {round((total_params / 1000000), 2)} M\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaConfig {\n",
       "  \"bos_token_id\": 0,\n",
       "  \"conv_kernel\": 4,\n",
       "  \"eos_token_id\": 0,\n",
       "  \"expand\": 2,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.1,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"mamba\",\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"rescale_prenorm_residual\": false,\n",
       "  \"residual_in_fp32\": true,\n",
       "  \"state_size\": 16,\n",
       "  \"time_step_floor\": 0.0001,\n",
       "  \"time_step_init_scheme\": \"random\",\n",
       "  \"time_step_max\": 0.1,\n",
       "  \"time_step_min\": 0.001,\n",
       "  \"time_step_rank\": 48,\n",
       "  \"time_step_scale\": 1.0,\n",
       "  \"transformers_version\": \"4.40.0.dev0\",\n",
       "  \"use_bias\": false,\n",
       "  \"use_cache\": true,\n",
       "  \"use_conv_bias\": true,\n",
       "  \"vocab_size\": 50280\n",
       "}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NewMamba(\n",
       "  (layers): ModuleList(\n",
       "    (0-7): 8 x MambaBlock(\n",
       "      (norm): MambaRMSNorm()\n",
       "      (mixer): MambaMixer(\n",
       "        (conv1d): Conv1d(512, 512, kernel_size=(4,), stride=(1,), padding=(3,), groups=512)\n",
       "        (act): SiLU()\n",
       "        (in_proj): Linear(in_features=256, out_features=1024, bias=False)\n",
       "        (x_proj): Linear(in_features=512, out_features=80, bias=False)\n",
       "        (dt_proj): Linear(in_features=48, out_features=512, bias=True)\n",
       "        (out_proj): Linear(in_features=512, out_features=256, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MambaConfig, MambaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "model_2 = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n",
    "# input_ids = tokenizer(\"Hey how are you doing?\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# out = model.generate(input_ids, max_new_tokens=10)\n",
    "# print(tokenizer.batch_decode(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaForCausalLM(\n",
       "  (backbone): MambaModel(\n",
       "    (embeddings): Embedding(50280, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x MambaBlock(\n",
       "        (norm): MambaRMSNorm()\n",
       "        (mixer): MambaMixer(\n",
       "          (conv1d): Conv1d(1536, 1536, kernel_size=(4,), stride=(1,), padding=(3,), groups=1536)\n",
       "          (act): SiLU()\n",
       "          (in_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (x_proj): Linear(in_features=1536, out_features=80, bias=False)\n",
       "          (dt_proj): Linear(in_features=48, out_features=1536, bias=True)\n",
       "          (out_proj): Linear(in_features=1536, out_features=768, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_f): MambaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>Q:\\n\\nHow to get the value of a variable in a function in C#\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(model.generate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Prediction Per Token Absolute Deviation from Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = (pred[0]-gt[0])[:,0][:]/(gt[0][:,0][:])\n",
    "a2 = (pred_2[0]-gt[0])[:,0][:]/(gt[0][:,0][:])\n",
    "a1 = a1.detach().cpu().numpy()\n",
    "a2 = a2.detach().cpu().numpy()\n",
    "b1 = (pred[0]-gt[0])[:,1][:]\n",
    "b2 = (pred_2[0]-gt[0])[:,1][:]\n",
    "b1 = b1.detach().cpu().numpy()\n",
    "b2 = b2.detach().cpu().numpy()\n",
    "plt.figure()\n",
    "plt.plot([i for i in range(39,150)],a1[39:150],label = 'model_1_delta_amplitude')\n",
    "plt.plot([i for i in range(39,150)],a2[39:150],label = 'model_2_delta_amplitude')\n",
    "plt.legend();plt.show()\n",
    "plt.figure()\n",
    "plt.plot([i for i in range(39,150)],b1[39:150],label = 'model_1_delta_phi')\n",
    "plt.plot([i for i in range(39,150)],b2[39:150],label = 'model_2_delta_phi')\n",
    "plt.legend();plt.show()\n",
    "plt.figure()\n",
    "# plt.plot([x for x in range(39,len(a1))],(a1-a2)[39:],label = 'amplitude_diff')\n",
    "plt.plot([x for x in range(39,150)],(b1-b2)[39:150],label = 'phi_diff')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fc, (mag_min, mag_max) in dm.test_dataloader():\n",
    "    break\n",
    "\n",
    "fc = fc.to('cuda')\n",
    "mag_min = mag_min.to('cuda')\n",
    "mag_max = mag_max.to('cuda')\n",
    "low_res = fc[:, flatten_order][:, :model.input_seq_length]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model.sres.forward_inference(low_res)\n",
    "\n",
    "lowres = torch.zeros_like(pred)\n",
    "lowres+=fc.min()\n",
    "lowres[:, :model.input_seq_length] = fc[:, model.dst_flatten_order][:, :model.input_seq_length]\n",
    "pred_img = model.convert2img(fc=pred, mag_min=mag_min, mag_max=mag_max)     \n",
    "lowres_img = model.convert2img(fc=lowres, mag_min=mag_min, mag_max=mag_max)\n",
    "gt_img = model.convert2img(fc=fc[:, model.dst_flatten_order], mag_min=mag_min, mag_max=mag_max)\n",
    "\n",
    "\n",
    "lowres_psnr = PSNR(gt_img,lowres_img)\n",
    "pred_psnr = PSNR(gt_img,pred_img)\n",
    "\n",
    "\n",
    "sns.histplot(pred_psnr.cpu().detach() - lowres_psnr.cpu().detach(), kde=True, color='green', legend= True )\n",
    "fig.legend()\n",
    "plt.savefig('psnr_diff.png')\n",
    "plt.close()\n",
    "for i in [pred_img, lowres_img, gt_img]:\n",
    "    i.cpu().detach().numpy()\n",
    "for sample in range(5):\n",
    "    fig2 = plt.figure(figsize=(31/2., 10/2.)) \n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[10,0.5, 10, 0.5, 10]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[2])\n",
    "    ax2 = plt.subplot(gs[4])\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                        hspace = 0, wspace = 0)\n",
    "\n",
    "    ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.imshow(lowres_img[sample].cpu(), cmap='gray')\n",
    "    ax0.set_title('Low-Resolution Input');\n",
    "    ax0.axis('equal');\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.imshow(pred_img[sample].cpu(), cmap='gray')\n",
    "    ax1.set_title('Prediction');\n",
    "    ax1.axis('equal');\n",
    "\n",
    "\n",
    "    ax2.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.imshow(gt_img[sample].cpu(), cmap='gray')\n",
    "    ax2.set_title('Ground Truth');\n",
    "    ax2.axis('equal');\n",
    "\n",
    "from sklearn.random_projection import sample_without_replacement\n",
    "\n",
    "\n",
    "diff = []\n",
    "for i in range(5):\n",
    "    sample = i\n",
    "    # pred_img[i] = (pred_img[i] - gt_img[i].min())/(gt_img[i].max() - gt_img[i].min())\n",
    "    # lowres_img[i] = (lowres_img[i] - gt_img[i].min())/(gt_img[i].max() - gt_img[i].min())\n",
    "    fig = plt.figure(figsize=(31/2., 10/2.))\n",
    "    gs = gridspec.GridSpec(1, 5, width_ratios=[10,0.5, 10, 0.5, 10]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax1 = plt.subplot(gs[2])\n",
    "    ax2 = plt.subplot(gs[4])\n",
    "    plt.subplots_adjust(top = 1, bottom = 0, right = 1, left = 0, \n",
    "                        hspace = 0, wspace = 0)\n",
    "    ax0.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax0.yaxis.set_major_locator(plt.NullLocator())\n",
    "    lowres_psnr = PSNR(gt_img, lowres_img)\n",
    "    # lowres_img_vs_gt_img_psnr = PSNR(gt_img[sample].cpu()[2:, 2:], lowres_img[sample].cpu()[2:, 2:], drange=torch.tensor(255., dtype=torch.float32))\n",
    "    ax0.imshow(np.roll(abs(torch.fft.rfftn(lowres_img[sample].cpu(),dim = [0,1])),13,0))\n",
    "    ax0.set_title('Low-Resolution Input');\n",
    "    ax0.set_xlabel(f'PSNR: {lowres_psnr[sample].cpu().detach()} dB')\n",
    "    ax0.axis('equal');\n",
    "\n",
    "    ax1.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax1.imshow(np.roll(abs(torch.fft.rfftn(pred_img[sample].cpu(),dim = [0,1])),13,0))\n",
    "    pred_psnr = PSNR(gt_img, pred_img)\n",
    "    # pred_vs_gt_img_psnr = PSNR(gt_img[sample].cpu()[2:, 2:], pred_img[sample].cpu()[2:, 2:], drange=torch.tensor(255., dtype=torch.float32))\n",
    "    ax1.set_title('Prediction');\n",
    "    ax1.set_xlabel(f'PSNR: {pred_psnr[sample].cpu().detach()} dB')\n",
    "    ax1.axis('equal');\n",
    "\n",
    "    ax2.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.yaxis.set_major_locator(plt.NullLocator())\n",
    "    ax2.imshow(np.roll(abs(torch.fft.rfftn(gt_img[sample].cpu(),dim = [0,1])),13,0))\n",
    "    ax2.set_title('Ground Truth');\n",
    "    # ax2.set_xlabel(f'Max: {gt_img[sample].cpu().max():.2f} Min: {gt_img[sample].cpu().min():.2f}');\n",
    "    ax2.axis('equal');\n",
    "    diff.append(lowres_psnr - pred_psnr)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "\n",
    "# import torch\n",
    "# import wandb\n",
    "# import ssl\n",
    "# import datetime\n",
    "import matplotlib\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from fit.utils.tomo_utils import get_polar_rfft_coords_2D\n",
    "# from fit.modules.SResTransformerModule import SResTransformerModule\n",
    "# from fit.datamodules.super_res.SResDataModule import MNIST_SResFITDM, CelebA_SResFITDM\n",
    "\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# from pytorch_lightning import Trainer, seed_everything\n",
    "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from fit.transformers_fit.PSNR import RangeInvariantPsnr as PSNR\n",
    "\n",
    "# ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# torch.set_float32_matmul_precision(\"medium\")\n",
    "# seed_everything(22122020)\n",
    "\n",
    "# # if __name__ == \"__main__\":\n",
    "# import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--causal_mask\", action=\"store_true\", help=\"Use causal mask\", default=True)\n",
    "# parser.add_argument(\"--d_query\", type=int, help=\"d_query\", default=32)\n",
    "# parser.add_argument(\"--model_type\", type=str, help=\"Model to be used in the transformer (torch or fast)\", default=\"fast\")\n",
    "# parser.add_argument(\"--n_layers\", type=int, help=\"Number of layers in the transformer\", default=8)\n",
    "# parser.add_argument(\"--n_heads\", type=int, help=\"No of heads in the transformer\", default=8)\n",
    "# parser.add_argument(\"--n_shells\",type=int,help=\"Number of shells used as lowres-input in the transformer\",default=5)\n",
    "# parser.add_argument(\"--models_save_path\", type=str, default=\"/home/aman.kukde/Projects/FourierImageTransformer/models/\")\n",
    "# parser.add_argument(\"--model_name\", type=str, default= 'Fast_prod__L_8_H_8_s_5_subset_False_27-03_16-58-36/epoch=222-step=383337.ckpt')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# n_layers = args.n_layers\n",
    "# n_shells = args.n_shells\n",
    "# n_heads = args.n_heads\n",
    "# model_type = args.model_type\n",
    "# models_save_path = args.models_save_path\n",
    "# model_name = args.model_name\n",
    "# causal_mask = args.causal_mask\n",
    "# d_query = args.d_query\n",
    "\n",
    "# dm = MNIST_SResFITDM(root_dir=\"./datamodules/data/\", batch_size=32)\n",
    "# dm.prepare_data()\n",
    "# dm.setup()\n",
    "\n",
    "# r, phi, flatten_order, order = get_polar_rfft_coords_2D(img_shape=dm.gt_shape)\n",
    "\n",
    "# model = SResTransformerModule(\n",
    "#     n_heads=n_heads,\n",
    "#     d_query=d_query,\n",
    "#     img_shape=dm.gt_shape,\n",
    "#     coords=(r, phi),\n",
    "#     model_type=model_type,\n",
    "#     dst_flatten_order=flatten_order,\n",
    "#     dst_order=order,\n",
    "#     weight_decay=0.01,\n",
    "#     n_layers=n_layers,\n",
    "#     num_shells=n_shells,\n",
    "# )\n",
    "# print(f\"\\n\\n\\n\\n{model}\\n\\n\\n\\n\")\n",
    "# weights = torch.load(models_save_path + model_name)['state_dict']\n",
    "# model.load_state_dict(weights)\n",
    "# print(f\"Model loaded successfully {model_name}\")\n",
    "\n",
    "# trainer = Trainer(max_epochs=100, \n",
    "#                 callbacks=ModelCheckpoint(\n",
    "#                                             dirpath=None,\n",
    "#                                             save_top_k=1,\n",
    "#                                             verbose=False,\n",
    "#                                             save_last=True,\n",
    "#                                             monitor='Validation/avg_val_loss',\n",
    "#                                             mode='min'\n",
    "#                                         ), \n",
    "#                 deterministic=True)\n",
    "\n",
    "model.cuda()\n",
    "\n",
    "def make_figs(lowres_psnr, pred_psnr):\n",
    "    font = {'family' : 'serif',\n",
    "        'weight': 'normal',\n",
    "        'size'   : 16}\n",
    "    matplotlib.rc('font', **font)\n",
    "    \n",
    "    fig = plt.figure(figsize = (12,9))\n",
    "    sns.histplot(lowres_psnr.cpu().detach(), kde=True, color='blue',legend =True,label = \"lowres\")\n",
    "    sns.histplot(pred_psnr.cpu().detach(), kde=True, color='red', legend= True, label = \"pred\")\n",
    "    fig.legend()\n",
    "    plt.savefig('../inference_results/psnr_hist.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure(figsize = (12,9))\n",
    "    sns.histplot(pred_psnr.cpu().detach() - lowres_psnr.cpu().detach(), kde=True, color='green', legend= True, label = \"diff\")\n",
    "    fig.legend()\n",
    "    plt.savefig('../inference_results/psnr_diff.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize = (12,9));\n",
    "    plt.boxplot([lowres_psnr,pred_psnr,pred_psnr - lowres_psnr],widths = [0.9]*3,labels = ['lowres_psnr','pred_psnr', 'diff (pred - lowres)']);\n",
    "    plt.savefig('../inference_results/psnr_box_LvsPvsD.png');\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize = (12,9));\n",
    "    plt.boxplot([lowres_psnr,pred_psnr],widths = [0.9]*2,labels = ['lowres_psnr','pred_psnr']);\n",
    "    plt.savefig('../inference_results/psnr_box_LvsP.png');\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize = (12,9));\n",
    "    plt.boxplot([pred_psnr - lowres_psnr],widths = [0.9],labels = ['diff (pred - lowres)']);\n",
    "    plt.savefig('../inference_results/psnr_box_diff.png');\n",
    "    plt.close()\n",
    "\n",
    "    diff = np.sort(pred - lowres)\n",
    "    p = np.arange(0, 101, 1)\n",
    "    xt = np.arange(0, 105, 5)\n",
    "    perc = np.percentile(diff, q=p)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(diff, label='PSNR Difference Prediction - Lowres')\n",
    "    plt.plot((len(diff)+1) * p/100., perc, 'ro',label = '+1 Percentile of PSNR Difference Distribution')\n",
    "\n",
    "    plt.xticks((len(diff)-1) * xt/100., map(str, xt))\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig('../inference_results/psnr_diff_percentile.png')\n",
    "    \n",
    "    return None\n",
    "lowres_psnr = []\n",
    "pred_psnr = []\n",
    "\n",
    "\n",
    "def get_lowres_pred_gt_img(fc, pred, mag_min, mag_max):\n",
    "    lowres = torch.zeros_like(pred)\n",
    "    lowres += fc.min()\n",
    "    lowres[:, :model.input_seq_length] = fc[:, model.dst_flatten_order][:, :model.input_seq_length]\n",
    "    pred_img = model.convert2img(fc=pred, mag_min=mag_min, mag_max=mag_max)     \n",
    "    lowres_img = model.convert2img(fc=lowres, mag_min=mag_min, mag_max=mag_max)\n",
    "    gt_img = model.convert2img(fc=fc[:, model.dst_flatten_order], mag_min=mag_min, mag_max=mag_max)\n",
    "    return pred_img,lowres_img,gt_img\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowres = torch.load('../inference_results/lowres.pt').cpu()\n",
    "pred = torch.load('../inference_results/pred.pt').cpu()\n",
    "make_figs(low_res,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
